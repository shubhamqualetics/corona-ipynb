{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing Necessary Libraries.\n",
    "import pandas as pd\n",
    "import os\n",
    "os.chdir('E:\\\\COVID\\\\NonAugmentedTrain')\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import keras  \n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign lables to each images present under specific classified folders.\n",
    "# also coverting all images to RGB format so that all images have one standard\n",
    "# Resolution may be different of images so converting all to 50,50 shape.\n",
    "\n",
    "data=[]\n",
    "labels=[]\n",
    "BacterialPneumonia=os.listdir(\"E:/COVID/NonAugmentedTrain/BacterialPneumonia/\")\n",
    "for a in BacterialPneumonia:\n",
    "    try:\n",
    "        image=cv2.imread(\"E:/COVID/NonAugmentedTrain/BacterialPneumonia/\"+a)\n",
    "        image_from_array = Image.fromarray(image, 'RGB')\n",
    "        size_image = image_from_array.resize((50, 50))\n",
    "        data.append(np.array(size_image))\n",
    "        labels.append(0)\n",
    "    except AttributeError:\n",
    "        print(\"\")\n",
    "        \n",
    "COVID=os.listdir(\"E:/COVID/NonAugmentedTrain/COVID-19/\")\n",
    "for b in COVID:\n",
    "    try:\n",
    "        image=cv2.imread(\"E:/COVID/NonAugmentedTrain/COVID-19/\"+b)\n",
    "        image_from_array = Image.fromarray(image, 'RGB')\n",
    "        size_image = image_from_array.resize((50, 50))\n",
    "        data.append(np.array(size_image))\n",
    "        labels.append(1)\n",
    "    except AttributeError:\n",
    "        print(\"\")\n",
    "        \n",
    "Normal=os.listdir(\"E:/COVID/NonAugmentedTrain/Normal/\")\n",
    "for c in Normal:\n",
    "    try:\n",
    "        image=cv2.imread(\"E:/COVID/NonAugmentedTrain/Normal/\"+c)\n",
    "        image_from_array = Image.fromarray(image, 'RGB')\n",
    "        size_image = image_from_array.resize((50, 50))\n",
    "        data.append(np.array(size_image))\n",
    "        labels.append(2)\n",
    "    except AttributeError:\n",
    "        print(\"\")\n",
    "\n",
    "ViralPneumonia=os.listdir(\"E:/COVID/NonAugmentedTrain/ViralPneumonia/\")\n",
    "for d in ViralPneumonia:\n",
    "    try:\n",
    "        image=cv2.imread(\"E:/COVID/NonAugmentedTrain/ViralPneumonia/\"+d)\n",
    "        image_from_array = Image.fromarray(image, 'RGB')\n",
    "        size_image = image_from_array.resize((50, 50))\n",
    "        data.append(np.array(size_image))\n",
    "        labels.append(3)\n",
    "    except AttributeError:\n",
    "        print(\"\")\n",
    "\n",
    "# labels given are 0,1,2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cells=np.array(data)\n",
    "labels=np.array(labels)\n",
    "\n",
    "# now cells consist of array of pixels of image and labels consist of corresponding class to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x900 with 49 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displaying images after classifing them, and converting numberic class to its distinguish categorical class\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(1 , figsize = (15 , 9))\n",
    "n = 0 \n",
    "for i in range(49):\n",
    "    n += 1 \n",
    "    r = np.random.randint(0 , Cells.shape[0] , 1)\n",
    "    plt.subplot(7 , 7 , n)\n",
    "    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n",
    "    plt.imshow(Cells[r[0]])\n",
    "    plt.title('BacterialPneumonia' if labels[r[0]] == 0 else ('COVID' if  labels[r[0]] == 1 else( 'Normal'  if labels[r[0]] == 2 else 'ViralPneumonia' )))\n",
    "    plt.xticks([]) , plt.yticks([])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=np.arange(Cells.shape[0])\n",
    "np.random.shuffle(s)\n",
    "Cells=Cells[s]\n",
    "labels=labels[s]\n",
    "\n",
    "# shuffling of images so that images should not get trained in serial class order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(np.unique(labels))\n",
    "len_data=len(Cells)\n",
    "\n",
    "# assign no. of class for model and determine length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "# making data for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,x_test)=Cells[(int)(0.1*len_data):],Cells[:(int)(0.1*len_data)]\n",
    "x_train = x_train.astype('float32')/255 # As we are working on image data we are normalizing data by divinding 255.\n",
    "x_test = x_test.astype('float32')/255\n",
    "train_len=len(x_train)\n",
    "test_len=len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1802, 50, 50, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 50, 50, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2002, 50, 50, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cells.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we don't have unseen data for validation purpose so we will take only train data for model building\n",
    "#instead of taking complete data\n",
    "\n",
    "#NOTE: Accuracy can be imporved in future by taking complete data for model building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign labels\n",
    "\n",
    "(y_train,y_test)=labels[(int)(0.1*len_data):],labels[:(int)(0.1*len_data)]\n",
    "A=y_test\n",
    "y_train=keras.utils.to_categorical(y_train,num_classes)\n",
    "y_test=keras.utils.to_categorical(y_test,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 0, 3, 3, 2, 3, 0, 0, 0, 2, 0, 0, 3, 2, 0, 2, 0, 2, 2,\n",
       "       2, 2, 2, 3, 2, 0, 3, 1, 0, 3, 0, 2, 2, 3, 2, 0, 3, 2, 0, 0, 2, 3,\n",
       "       3, 0, 3, 3, 2, 2, 3, 3, 0, 2, 2, 2, 2, 0, 2, 2, 0, 3, 2, 1, 3, 3,\n",
       "       2, 3, 3, 1, 2, 2, 3, 0, 0, 1, 2, 2, 3, 0, 0, 0, 0, 2, 2, 1, 0, 2,\n",
       "       2, 3, 2, 0, 2, 3, 3, 2, 2, 3, 3, 2, 2, 0, 0, 3, 2, 3, 2, 0, 3, 0,\n",
       "       1, 0, 2, 3, 0, 2, 0, 3, 3, 2, 0, 2, 3, 2, 0, 0, 3, 0, 2, 2, 3, 0,\n",
       "       3, 2, 2, 2, 3, 2, 2, 0, 2, 2, 0, 0, 0, 2, 2, 3, 2, 2, 2, 0, 0, 2,\n",
       "       0, 2, 2, 0, 3, 0, 3, 2, 3, 3, 2, 2, 2, 0, 2, 0, 0, 0, 0, 1, 0, 2,\n",
       "       0, 2, 2, 0, 2, 0, 1, 2, 2, 0, 3, 0, 3, 3, 0, 2, 1, 0, 2, 2, 0, 3,\n",
       "       0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saved classes before transforming it to array\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 50, 50, 16)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 25, 25, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 25, 25, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               1152500   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 2004      \n",
      "=================================================================\n",
      "Total params: 1,165,048\n",
      "Trainable params: 1,165,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#creating sequential model\n",
    "# Accuracy can be improve in future by changing parameters and no. assign to parameters\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500,activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4,activation=\"softmax\"))# represent output layer neurons \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 1802 samples, validate on 200 samples\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1802/1802 [==============================] - 7s 4ms/step - loss: 1.0210 - acc: 0.5427 - val_loss: 0.8399 - val_acc: 0.6150\n",
      "Epoch 2/15\n",
      "1802/1802 [==============================] - 6s 3ms/step - loss: 0.6512 - acc: 0.7370 - val_loss: 0.6757 - val_acc: 0.7150\n",
      "Epoch 3/15\n",
      "1802/1802 [==============================] - 7s 4ms/step - loss: 0.5380 - acc: 0.7780 - val_loss: 0.5794 - val_acc: 0.7000\n",
      "Epoch 4/15\n",
      "1802/1802 [==============================] - 7s 4ms/step - loss: 0.4962 - acc: 0.7913 - val_loss: 0.5590 - val_acc: 0.7100\n",
      "Epoch 5/15\n",
      "1802/1802 [==============================] - 7s 4ms/step - loss: 0.5052 - acc: 0.7786 - val_loss: 0.5292 - val_acc: 0.7500\n",
      "Epoch 6/15\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 0.4612 - acc: 0.7913 - val_loss: 0.5239 - val_acc: 0.7250\n",
      "Epoch 7/15\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 0.4392 - acc: 0.8130 - val_loss: 0.4891 - val_acc: 0.7400\n",
      "Epoch 8/15\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 0.4065 - acc: 0.8235 - val_loss: 0.4660 - val_acc: 0.7700\n",
      "Epoch 9/15\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 0.3900 - acc: 0.8224 - val_loss: 0.5136 - val_acc: 0.7450\n",
      "Epoch 10/15\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 0.3777 - acc: 0.8269 - val_loss: 0.5199 - val_acc: 0.7400\n",
      "Epoch 11/15\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 0.3917 - acc: 0.8224 - val_loss: 0.4886 - val_acc: 0.7500\n",
      "Epoch 12/15\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 0.3573 - acc: 0.8485 - val_loss: 0.4956 - val_acc: 0.7500\n",
      "Epoch 13/15\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 0.3610 - acc: 0.8324 - val_loss: 0.4580 - val_acc: 0.7650\n",
      "Epoch 14/15\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 0.3579 - acc: 0.8396 - val_loss: 0.4415 - val_acc: 0.7650\n",
      "Epoch 15/15\n",
      "1802/1802 [==============================] - 5s 3ms/step - loss: 0.3671 - acc: 0.8302 - val_loss: 0.4657 - val_acc: 0.7650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff8107c2c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,batch_size=50,epochs=15,validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy can be again improved by increasing number of epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 860us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.465653567314148, 0.765]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities for test set\n",
    "yhat_probs = model.predict(x_test, verbose=0)\n",
    "# predict classes for test set\n",
    "yhat_classes = model.predict_classes(x_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 3, 3, 0, 2, 0, 3, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2,\n",
       "       2, 2, 2, 0, 2, 0, 3, 1, 0, 0, 0, 2, 2, 3, 2, 3, 0, 2, 3, 0, 2, 0,\n",
       "       0, 0, 3, 0, 2, 2, 0, 3, 0, 2, 2, 2, 2, 0, 2, 2, 3, 0, 2, 1, 0, 0,\n",
       "       2, 0, 0, 1, 2, 3, 0, 0, 0, 1, 2, 2, 3, 0, 0, 0, 3, 2, 2, 1, 0, 2,\n",
       "       2, 3, 2, 0, 2, 0, 0, 2, 2, 3, 0, 2, 2, 3, 0, 0, 2, 0, 2, 0, 0, 0,\n",
       "       1, 0, 2, 3, 3, 2, 0, 0, 3, 2, 0, 2, 3, 2, 0, 0, 3, 3, 2, 2, 0, 0,\n",
       "       0, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 0, 0, 2, 2, 1, 2, 2, 2, 0, 0, 2,\n",
       "       0, 2, 2, 0, 0, 0, 0, 2, 3, 3, 2, 2, 2, 0, 2, 0, 0, 0, 0, 1, 3, 2,\n",
       "       0, 2, 2, 0, 2, 0, 1, 2, 2, 3, 0, 0, 3, 3, 3, 2, 0, 0, 2, 2, 0, 2,\n",
       "       3, 1], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "yhat_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 0, 3, 3, 2, 3, 0, 0, 0, 2, 0, 0, 3, 2, 0, 2, 0, 2, 2,\n",
       "       2, 2, 2, 3, 2, 0, 3, 1, 0, 3, 0, 2, 2, 3, 2, 0, 3, 2, 0, 0, 2, 3,\n",
       "       3, 0, 3, 3, 2, 2, 3, 3, 0, 2, 2, 2, 2, 0, 2, 2, 0, 3, 2, 1, 3, 3,\n",
       "       2, 3, 3, 1, 2, 2, 3, 0, 0, 1, 2, 2, 3, 0, 0, 0, 0, 2, 2, 1, 0, 2,\n",
       "       2, 3, 2, 0, 2, 3, 3, 2, 2, 3, 3, 2, 2, 0, 0, 3, 2, 3, 2, 0, 3, 0,\n",
       "       1, 0, 2, 3, 0, 2, 0, 3, 3, 2, 0, 2, 3, 2, 0, 0, 3, 0, 2, 2, 3, 0,\n",
       "       3, 2, 2, 2, 3, 2, 2, 0, 2, 2, 0, 0, 0, 2, 2, 3, 2, 2, 2, 0, 0, 2,\n",
       "       0, 2, 2, 0, 3, 0, 3, 2, 3, 3, 2, 2, 2, 0, 2, 0, 0, 0, 0, 1, 0, 2,\n",
       "       0, 2, 2, 0, 2, 0, 1, 2, 2, 0, 3, 0, 3, 3, 0, 2, 1, 0, 2, 2, 0, 3,\n",
       "       0, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling back actuals\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.79      0.69        62\n",
      "           1       0.90      0.90      0.90        10\n",
      "           2       0.99      0.98      0.98        81\n",
      "           3       0.53      0.34      0.42        47\n",
      "\n",
      "    accuracy                           0.77       200\n",
      "   macro avg       0.76      0.75      0.75       200\n",
      "weighted avg       0.76      0.77      0.75       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(A, yhat_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.765000\n",
      "Precision: 0.760146\n",
      "Recall: 0.765000\n",
      "F1 score: 0.754059\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(A, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "precision = precision_score(A, yhat_classes,average='weighted')\n",
    "print('Precision: %f' % precision)\n",
    "recall = recall_score(A, yhat_classes,average='weighted')\n",
    "print('Recall: %f' % recall)\n",
    "f1 = f1_score(A, yhat_classes,average='weighted')\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COVID cases are able to be classified with 90% of assurance so keeping this model as best solution for now.\n",
    "Where as F1 score for class 3 (Viral Pneumonia) is low but not a problem as they are getting misclassified but are getting \n",
    "classified to class 1 which is Bacterical Pneumonia. Hence as per domain both cases belongs to same family and hence classifing them as one of the alternate class is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 1. To increase F1 score of COVID, duplicate images were made in the image folder itself, but didn't change the F1 score much so the approach was discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Augmentation technique was performed on the images but that increased the number of images for other class because of which F1 score came down. Hence approch was removed.\n",
    "Where as performing augmentation on covid class didn't help to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.The code was rerun again because of which accuracy went down.\n",
    "The model which was deployed got a accuracy of 85% and the F1 score was about 80%.\n",
    "\n",
    "Reason:\n",
    "Neural network algorithms are stochastic. This means they make use of randomness, such as initializing to random weights, and in turn the same network trained on the same data can produce different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. COVID cases images were only 60 in data available of which model is train only about 50 images.\n",
    "Hence the result predicted by the model for real unseen data may not be correct as model in not trained on sufficient evidences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
